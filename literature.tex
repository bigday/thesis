\chapter{Related work}

\fixme{Refer to relevant sources a topic at a time}

% libraries
% \citep{OpenNI} \citep{NITE} \citep{PCL}
\section{Point cloud alignment} \label{literature.alignment}

\fixme{mention rigid and non-rigid registration}

\fixme{Why is point cloud alignment interesting?}

The Iterative Closest Point (ICP) algorithm is commonly used for aligning two point clouds. \fixme{The algorithm was originally presented by \citet{besl1992method} and \citet{chen1991object}} The algorithm in pseudocode is shown in Algorithm \ref{literature.pseudoicp}. It works by finding corresponding points from the two point clouds. Then a rigid transformation that minimizes the error between the corresponding point pairs is searched for. This transformation is then applied. Iterating this multiple times, a local optimum should be found.

\begin{algorithm}
\caption{Iterative Closest Point}
\label{literature.pseudoicp}
\begin{algorithmic}
    \For{1..iterations}
        \ForAll{point $x_i$ in X}
            \State $y_i$ = closest point in Y
        \EndFor
        \State error := $sum_i (x_i-y_i)^2$
        \State find rigid transformation $T_i$ that minimizes error
        \State apply $T_i$ on X
    \EndFor
    \State \Return output T = $product_i T_i$
\end{algorithmic}
\end{algorithm}

Even from this description it's obvious that ICP has its shortcomings. If the initial guess is too much off, the algorithm can yield bad results. This is because for non-trivial point clouds, local optima may exist that are significantly different from the global optimum.

Another issue with ICP is that it requires a lot of computation if the point clouds are large. \citet{rusinkiewicz2001efficient} review different ways of improving the efficiency of ICP. They also attain minor improvements to the alignment accuracy.

However, even these improved variants of ICP suffer from converging on local minima. The EM-ICP \fixme{(abbreviation)} algorithm introduced by \citet{granger2006multi} overcomes this problem \fixme{at the cost of being computationally very intensive. (is this accurate?)}

\fixme{\citet{tamaki2010softassign} make EM-ICP more efficient by implementing the computation on GPU using CUDA. The speed is unfortunately still not fast enough for real-time usage on current hardware.}

Another interesting improvement over ICP is suggested by \citet{tykkala2011direct}. Their algorithm, Direct Iterative Closest Point, is very well suited for Simultaneous Localization and Mapping (SLAM) using an RGB-D sensor. Direct ICP stores all data in 2D images, thus avoiding expensive 3D nearest neighbor searches. This makes it computationally much cheaper than traditional ICP. The change in approach from 3D point coordinate matching to computing transformations between 2D images also enables using RGB as well as depth data for computation. Using both simultaneously can make the algorithm more robust and accurate. This is demonstrated for deriving the trajectory and pose of a moving camera in static scenery. The downside is that a balancing factor $\lambda$ between the visual and depth data needs to be chosen.

\citep{chui2003new} TPS-RPM \citep{yang2011thin}

The algorithm developed by \citet{huang2011visual} is not point cloud alignment as such, but as it is used to replace ICP in some cases \citep{Whelan12rssw}, it deserves a mention. The article leaves the method unnamed, but an implementation called FOVIS\footnote{\url{http://code.google.com/p/fovis}} is made freely available by the authors. \citet{huang2011visual} aim to create a robust system for stabilizing and controlling an autonomous micro air vehicle (MAV). Their chosen hardware is a quadrotor MAV that carries a stripped-down Kinect and a small flight computer comprising a 1.86GHz Intel Core 2 Duo processor and 4GB of RAM (random access memory \fixme{(is this necessary?}). For autonomous flight, \citeauthor{huang2011visual} develop a visual odometry algorithm that is capable of tracking camera pose and trajectory in real time on the aforementioned hardware--more specifically, computation takes roughly 25ms per frame. The algorithm is image-based and uses RGB-D input, of which the RGB image is converted to grayscale. To enable feature detection at different scales, a Gaussian pyramid is created--in practice, a series of scaled-down images with Gaussian blur. The \fixme{FAST feature detector (source, acronym)} is used to extract features at each scale level. Features are matched between frames by computing sum of absolute differences of their feature descriptors, choosing the minimally different features as matches. An initial rotation estimate is calculated by a simple direct pixel error minimization method presented by \fixme{Mei et al}, and used to restrict feature matches only to the ones that correspond to the general rotation. The matches are further refined by finding the maximal amount of features that are consistent with each other given their locations (using the depth data), assuming a rigid, static scenery. After the feature matching is complete, the locations of each feature are used to find the new camera pose by minimizing feature reprojection error. This error measure is used to still drop possible outlier features, and the pose optimization is done again. Additionally, keyframes are used to reduce local drift in trajectory. As long as the camera motion between keyframe and current frame can be computed with enough inlier features, this result is also considered. When this is no longer possible, the current frame is set as the new keyframe. \citep{huang2011visual}


\section{Human body models}

Human anatomy is the obvious basis for any artificial model of the human body. Anatomy is not a simple matter; the main anatomical features according to \fixme{Gray's Anatomy} \citep{pick1977gray} are the skeleton, the articulations, muscles and fasciae, the blood-vascular system, the nervous system and the organs (grouped into six categories). Considering every detail is very impractical--a visually convincing human can be drawn without taking the nervous system into account, for example.

Even though a simplified view of anatomy is enough for visually representing humans, the basics are important. Anatomical knowledge of humans has been employed by visual artists at least since the days of Leonardo da Vinci, who did dissections to gain understanding of the human body \fixme{source}. Nowadays, several guides to human anatomy for artists have been published.

\fixme{TODO: what's important to artists?}
\citep{loomis1943figure} 


\fixme{choose the subsection for the following}

\textit{Morphable models} are defined by \citet{schneider2010fitting} to mean parametric models of 3D shape, created by applying Principal Component Analysis (PCA) on a database of meshes. As a single PCA model can only cover a limited subspace of shapes, complex shapes can be split into parts, creating in essence a multi-PCA model of the overall shape. Fitting such a morphable model to a shape is easy if the target representation is semantically consistent with the model. That is, the target should be a mesh that has the same number of vertices in positions corresponding to the model. Practically, this is rarely true, and a method for finding the correspondences is needed. \citet{schneider2010fitting} describe the classical way of fitting a morphable model to a point cloud that consists of three steps. First, a template mesh corresponding to the vertex structure of the morphable model must be fitted to the point cloud using a non-rigid registration method. This template is used to find points corresponding to vertices. The topology is then solved and the morphable model can be fitted. A better approach is suggested, where all the pose and shape constraints are written as a matrix equation. Then an iterative optimization is run, where the pose is optimized by an ICP-like method, parameter changes are solved from the matrix equation using singular value decomposition and then the parameters are updated. The optimization is stopped when the mean squared error becomes stable between iterations.

A morphable human head model and laser scans of faces are used as an example to show that simultaneous optimization indeed converges faster and produces a more accurate result than using an initial pose optimization and then only fitting shape. Additionally, the 3D head model fitting is shown to work with a mugshot-type set of two 2D images from two directions, by adding more equations to solve for depth.

The Shape Completion and Animation of People (SCAPE) method developed by \citet{anguelov2005scape} is a data-driven framework for creating a morphable human body model. The model is made low dimensional by decoupling the pose and body shape. The pose model must be learned first by comparing data of the same person in different poses. PCA is used to find deformations for each triangle in the mesh as the function of joint angles. To incorporate body shape variation, the position of each triangle is then defined as the product of pose deformation, body shape deformation and the rotation of the corresponding joint. Given data of different people in arbitrary poses, PCA is used to find a descriptive subspace of body shapes. The model built using these two components is shown to be directly applicable to creating 3D animation. Generating a mesh from parameters took about 1 second when the paper was published in 2005, leading the authors to suggest the method could allow real-time animation in the future. Another demonstrated application is shape completion (for example, from a partial view of a human)--an optimization process for finding pose and shape parameters from incomplete data is derived. Qualitatively, the model seems very capable of approximating the complete 3D human shape from a single incomplete point cloud.

The main limitation of models generated by SCAPE is that the correlation between body shape and muscle deformation cannot be modeled, as the two are decoupled. Practically this means that bodies with a lot of muscular tissue show deformations similar to those with a lot of fatty tissue. Moreover, the deformations are only dependent on joint angles, meaning flexed and relaxed muscles can't be differentiated in the same pose--this also restricts face modeling, as facial expressions are caused by muscular activity, not joint rotations. \citep{anguelov2005scape}

\citep{baek2012parametric}


\subsection{MakeHuman}

\fixme{should this be here?}

MakeHuman\footnote{\url{http://www.makehuman.org/}} is an open source software application for 3D human modeling.

MakeHuman has an androgynous base mesh that supposedly depicts an average human. It can be modified in diverse ways using a number of predefined `targets,' i.e. deformations. A commendable amount of work has been put into refining the base mesh and targets. Initially it seems like an arbitrary human could be accurately modeled using MakeHuman, given enough time for fine-tuning.

\section{3D reconstruction}

\citep{fabio2003point}

The Master of Research dissertation by \citet{charpentier2011accurate} makes an overview of possible methods at creating avatars with Kinect. Existing methods at creating both sprite-based and 3D avatars are discussed and compared. The working hypothesis is that a Kinect can be used for 3D scanning oneself and making a rigged avatar from the scan. This would allow a much more accurate representation of the user than any of the existing avatar creation tools. Creating a human model is shown to be possible by scanning the person in a static pose from different directions and manually aligning the scans in MeshLab\footnote{MeshLab is an open source application useful for working with point clouds and meshes, available at \url{http://meshlab.sourceforge.net/}}. These aligned scans are then used to create a mesh, which can then be rigged using the Pinocchio software, the prototype implementation of the automatic rigging method introduced by \citep{baran2007automatic}. The result is an animatable avatar that mostly works--however, in the particular case shown one joint is misplaced and there is a long strip of surface between the legs. Texturing is not attempted, and is noted to be slightly problematic due to color differences between the different point clouds. Coloring each vertex using the nearest point color is mentioned as a possible way of texturing the generated mesh. \citep{charpentier2011accurate}

The demonstrated avatar creation method is unwieldy considering that manual alignment takes hours to do. Automatic methods are explored, but no suitable one is found. \citeauthor{charpentier2011accurate} suggests that using KinectFusion to generate the human mesh should make the process easy, if the application was available. \citep{charpentier2011accurate}


Currently, the most promising work at creating personalized human avatars has been by \citet{weiss2011home}. Their approach uses the SCAPE body model \citep{anguelov2005scape}. This makes for very good reconstruction accuracy and excellent \fixme{rendered shapes (muscle deformation according to pose etc)}. However, the evaluation is slow\fixme{ at approximately 65 minutes for a human model, according to \citet{tongscanning}} and the SCAPE model needs to be trained with a large amount of data beforehand.

\citet{ahmed2005automatic} presented a method for creating human avatars from multi-view video. The process is automatic and the avatars are visually quite accurate. For capturing, a frame-synchronized setup of eight cameras is used. The human is segmented from each view, yielding eight silhouettes. A template body model is deformed so that it optimally fits all the silhouettes. The template consists of 16 parts, each of which is controlled by four B-splines. The deformations are optimized for each viewpoint, and for improved accuracy, using several non-subsequent frames. According to the authors, about five automatically chosen time steps are sufficient, and thus just a few seconds of multi-view video is required. The computation of the human model takes about 15 minutes. After the shape is personalized, it is textured by projecting the images back to the constructed model. The method takes occlusion into account and combines textures over several time steps. The texture reconstruction takes around 40 seconds. As an additional feature, the user can manually select frames with interesting facial expressions. These expressions can then be changed on the avatar. \citep{ahmed2005automatic}

\citet{stoll2010video} capture animatable virtual humans from multi-view video similar to what \citet{ahmed2005automatic} use, making the reconstruction more accurate and adding cloth modeling, while not implementing texturing. They show algorithms for identifying cloth and estimating its parameters for cloth simulation, and guessing the surface geometry underneath the clothing. The cloth capture and simulation is shown to be plausible by a user study. Computation of the model takes 2.5 hours for 1000 frames of multi-view video, but afterwards the model can be animated in real time using a rigged skeleton. \citep{stoll2010video}

\citet{tongscanning} use three Kinects and a turntable to scan human bodies. Their method requires the human to be standing in a static pose for 30 seconds, while the turntable rotates 360 degrees. The method starts by creating a very simplified template mesh of the human in first frame. This mesh is used to find the changes between each consecutive pair of frames. After the full rotation, the error sum of the pairwise registration can be calculated, and it is spread evenly across all the frames--similar to how loop closure works in SLAM systems. The authors note a better accuracy than \citep{weiss2011home}, which is due to the latter using a human shape model (SCAPE) that is only applicable to near-naked human bodies. In practice, the method by \citeauthor{tongscanning} is capable of modeling arbitrary surfaces on top of the human such as clothes and hair. It can also work with shapes that are not included in the SCAPE space, such as specific facial features not interpolatable from the SCAPE training data. However, this comes at the cost of animation. Whereas \fixme{SCAPE supports freeform posing and has realistic deformations \citep{anguelov2005scape},} \citet{tongscanning} only note that the ``skeleton and skin weights of the reconstructed body mesh can be automatically extracted'' using another method such as \citep{baran2007automatic}.

\fixme{The work of \citet{hirshbergc2011evaluating} focuses on aligning human laser scans from different directions. They develop methods for automatically locating the two ankles and the sellion landmark, defined as ``Point of greatest indentation of the nasal root depression'' \citep{blackwell2002civilian}. These three points are shown to be enough for accurate scan alignment of data in the CAESAR dataset \citep{robinette2002civilian}.}

(human pose)
\citep{baak2011data}
\citep{pekelny2008articulated}
\citep{droeschel20113d}

\section{KinectFusion}

KinectFusion is a real-time 3D reconstruction system that uses data from a moving Kinect sensor. It performs camera position and pose tracking, surface reconstruction and rendering. \citep{izadi2011kinectfusion}

The implementation works on consumer hardware by way of general-purpose computing on graphics processing units (GPGPU). More specifically, KinectFusion utilizes fully parallel tracking and mapping algorithms that are very efficient on the GPU (graphics processing unit) \citep{newcombe2011kinectfusion}.

\fixme{KinectFusion technology: raycasting, ICP, TSDF/voxel grid}

\fixme{KinectFusion uses a variation of Signed Distance Function (SDF), whose value is defined as the distance to the nearest surface for a point in space. The values are signed--a negative value means the point is inside an object. In the KinectFusion implementation, a 3D volume of fixed physical dimensions and resolution is allocated. For each cell in this voxel grid, the distance to nearest surface is saved. Actually, only values for a truncated area near the surface are actually stored--this variation is called Truncated Signed Distance Function (TSDF). \citep{izadi2011kinectfusion}}

KinectFusion was made in a project at Microsoft Research. No source code has been published. Nevertheless, an open source implementation called Kinfu has been made based on the descriptions in the two articles. Kinfu is available as part of the Point Cloud Library (PCL) \citep{PCL}.

The availability of an open source implementation has allowed further improvements on the system. For example, the KinectFusion algorithm was soon improved to work over extended areas \citep{Whelan12rssw}. The enhanced version is called Kintinuous and has been built on Kinfu. The authors have pledged to release the source code back to PCL \fixme{(citation)}, but as of writing this has not yet happened. The main contribution of \citet{Whelan12rssw} is to allow moving the TSDF grid, in essence removing the main restriction of KinectFusion that only allows reconstruction of a fixed-size cube (for example, $(3 \mathrm{m})^3$). This is implemented by making the TSDF data structure cyclical--the memory is pre-allocated, but the origin is given by a pointer and can be moved. A threshold is chosen, and when any of the camera coordinates ($x, y, z$) exceeds this threshold, the TSDF is moved. The origin moves to the new camera coordinates. The region no longer in the TSDF is raycasted to find surfaces, which are saved in the main memory. Cells in the TSDF memory that fall outside the new TSDF are marked unmapped, and will be reused to describe a new position. Mesh reconstruction utilizes the algorithm described in \fixme{(source)}, resulting in a high-polygon surface. The size of the surface is only limited by available memory. \citep{Whelan12rssw}

Another issue pointed out in KinectFusion is that ICP cannot work without geometric features. This shortcoming is alleviated by including FOVIS, the visual odometry algorithm as an alternative to ICP. This is shown to work in real-time, though its CPU implementation is slightly slower than the GPU implementation of ICP. \citep{Whelan12rssw}

Kintinuous was further improved by \citet{Whelan12tr} by adding an image-based tracking algorithm similar to \citep{tykkala2011direct} and the initial step in \citep{huang2011visual}. The three different algorithms used for odometry (ICP, FOVIS and image-based ``RGBD'') can be combined. ICP and RGBD combined can work in real-time and yield a more accurate trajectory than any of the algorithms alone. \citep{Whelan12tr}

Features still lacking in Kintinuous include loop closure and map reintegration. In practice, moving the sensor back to an area already visited is problematic. Since no map reintegration is implemented, the area will be overwritten. Drift may also be significant---no intelligent measures are taken to correct the error when an area is revisited.
